import gradio as gr

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough

import os
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings

from openai import OpenAI

from agent import getDocumentCharged

from langchain_openai import ChatOpenAI
from langfuse.callback import CallbackHandler


# Carga las variables de entorno desde el archivo .env
load_dotenv()
# Accede a la API key utilizando os.environ
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
LANGFUSE_PRIVATE_API_KEY = os.environ.get("LANGFUSE_PRIVATE_API_KEY")
LANGFUSE_PUBLIC_API_KEY = os.environ.get("LANGFUSE_PUBLIC_API_KEY")


handler = CallbackHandler(LANGFUSE_PUBLIC_API_KEY, LANGFUSE_PRIVATE_API_KEY)

model = ChatOpenAI(
    model="cohere/command-r-plus",
    temperature=0,
    max_tokens=1024,
    openai_api_key=OPENROUTER_API_KEY,
    base_url="https://openrouter.ai/api/v1",
    callbacks=[handler]
)


embeddings = HuggingFaceBgeEmbeddings(
    model_name="BAAI/bge-large-en",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': False}
)


load_vector_store = Chroma(
    persist_directory="stores/ConserGPT/", embedding_function=embeddings)
retriever = load_vector_store.as_retriever(search_kwargs={"k": 3})


# Provide a template following the LLM's original chat template.
template = """Utiliza la siguiente informaci칩n para responder a la pregunta del usuario.
Si no sabes la respuesta, di simplemente que no la sabes, no intentes inventarte una respuesta.
Si en "Contexto" recibes un array vac칤o [], significa que no hay informaci칩n relevante para responder a la pregunta.

Contexto:
{context} 

Pregunta: {question}

Devuelve la respuesta 칰til que aparece a continuaci칩n, incluyendo la fuente de la informaci칩n (source), de una forma adecuada usando el formato Markdown.

Responde solo y exclusivamente con la informaci칩n que se te ha sido proporcionada.
Responde siempre en castellano.

Solo si el usuario te pregunta por el n칰mero de archivos que hay cargados, ejecuta el siguiente c칩digo: {ShowDocu}, en caso contrario, omite este paso y no lo ejecutes.

Respuesta 칰til:"""

prompt = ChatPromptTemplate.from_template(template)

chain = (
    {"context": retriever, "question": RunnablePassthrough(
    ), "ShowDocu": RunnableLambda(getDocumentCharged)}
    | prompt
    | model
    | StrOutputParser()
)


def get_response(input):
    query = input
    output = chain.invoke(query)
    return output


# Define las opciones para las preguntas
preguntas = ["쮺u치l es el prop칩sito principal del Plan de Lectura y Bibliotecas Escolares en los Centros Educativos P칰blicos de Andaluc칤a?",
             "쮺u치l es el criterio principal para designar al coordinador del programa 'El Deporte en la Escuela' en los centros docentes p칰blicos de Andaluc칤a?", "쮺u치l es uno de los objetivos prioritarios de la pol칤tica educativa andaluza?"]

iface = gr.Interface(
    fn=get_response,
    inputs="text",
    examples=preguntas,
    outputs="markdown",
    title="游닀 ConserGPT 游닀",
    description="This is a RAG implementation based on Command R+.",
    allow_flagging='never',
)


iface.launch(share=True)
